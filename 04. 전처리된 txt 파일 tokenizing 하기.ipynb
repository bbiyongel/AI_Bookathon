{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 전처리된 txt 파일 tokenizing 하기\n",
    "- 작성 : 정민정 (https://github.com/jeina7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 전처리 된 txt 파일을 `tokenization.py` 파일로 토크나이징 한다.\n",
    "\n",
    "\n",
    "- 토크나이징 된 numpy array는 `data.hdf5` 파일로 저장한다.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 저장된 데이터 `data.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“야, 이 썩을 놈아! 시끄럽다고 몇 번이나 말하노? 어이?”\\n현관문을 부서져라 두드리며 욕할매가 고함을 친다. 나는 그러거나 말거나 문도 열어보지 않는다. 저러다가 시들해져 집에 간 적이 많아 이제 신경도 안 쓴다. 욕할매는 우리 아래층, 401호에 산다. 80살도 넘었다는데 목소리가 어찌나 우렁찬지 밖에서 소리를 지르면 우리 집이 쿵쿵 울리는 것 같다. 우리가 이사 오던 날부터 이삿짐 나르는 사다리차가 시끄럽다고 난리였다.\\n“허구헌 날 다 나뚜고 우짠 일로 이 늦은 시간에 이사를 해 쌌소?”\\n“죄송해요. 저희 부부가 다 일을 다니다 보니 이 시간에 할 수밖에 없네요.”\\n우리가 이사를 하는 시간이 저녁때인 것은 맞다. 하지만 깜깜한 한밤중도 아닌데 뭐 그리 난리 법석을 할 일은 아니다. 욕할매는 이삿짐 나르는데 불쑥 들어와서 한바탕 하고 갔다. 그 일이 있은 후로 우리 집 가훈은 ‘살금살금, 조용조용’이 된 것 같았다.\\n“아래층 할머니 너무 하신 거 아녜요?”\\n엄마는 부엌을 정리하'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './Crawling/textcrawler/text_data/data.txt'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. 단어 사전 `vocab.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.src.tokenization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tokens: 112074 \n",
      "\n",
      "[PAD]: 0\n",
      "[UNK]: 1\n",
      "[CLS]: 2\n",
      "[SEP]: 3\n",
      "[MASK]: 4\n",
      "Lyon: 5\n",
      "유리수: 6\n",
      "두호: 7\n",
      "##해온: 8\n",
      "나락: 9\n",
      "볼모: 10\n",
      "##정기예금: 11\n",
      "출조: 12\n",
      "##디자이너: 13\n",
      "호이안: 14\n",
      "##adintext: 15\n",
      "##습의: 16\n",
      "토왕: 17\n",
      "##통역장교: 18\n",
      "##초항: 19\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"./common/models/345K/vocab.txt\"\n",
    "vocab = load_vocab(vocab_path)\n",
    "print('all tokens:', len(vocab), '\\n')\n",
    "\n",
    "cnt = 1\n",
    "for k, v in vocab.items():\n",
    "    print(\"{}: {}\".format(k, v))\n",
    "    if cnt == 20: break\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vocab.txt`는 모든 단어들의 id가 담겨있다\n",
    "\n",
    "\n",
    "- 이걸 활용해서 모든 token들을 id로 변환함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. plain text를 전처리 할 `FullTokenizer` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 과정을 test 해보기 위해 300자를 자른 sample text를 활용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“야, 이 썩을 놈아! 시끄럽다고 몇 번이나 말하노? 어이?”\\n현관문을 부서져라 두드리며 욕할매가 고함을 친다. 나는 그러거나 말거나 문도 열어보지 않는다. 저러다가 시들해져 집에 간 적이 많아 이제 신경도 안 쓴다. 욕할매는 우리 아래층, 401호에 산다. 80살도 넘었다는데 목소리가 어찌나 우렁찬지 밖에서 소리를 지르면 우리 집이 쿵쿵 울리는 것 같다. 우리가 이사 오던 날부터 이삿짐 나르는 사다리차가 시끄럽다고 난리였다.\\n“허구헌 날 다 나뚜고 우짠 일로 이 늦은 시간에 이사를 해 쌌소?”\\n“죄송해요. 저희 부부가 다 일을 다'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[:300]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', '야', ',', '이', '썩', '##을', '놈', '##아', '!', '시끄럽', '##다고', '몇', '번이', '##나', '말', '##하', '##노', '?', '어', '##이', '?', '”', '현관문', '##을', '부서', '##져', '##라', '두드리', '##며', '욕', '##할매', '##가', '고함', '##을', '친다', '.', '나', '##는', '그러', '##거나', '말', '##거나', '문도', '열어', '##보', '##지', '않', '##는다', '.', '저러', '##다가', '시들', '##해져', '집', '##에', '간', '적', '##이', '많', '##아', '이제', '신경', '##도', '안', '쓴다', '.', '욕', '##할매', '##는', '우리', '아래층', ',', '401', '##호에', '산다', '.', '80', '##살', '##도', '넘', '##었', '##다는데', '목소리', '##가', '어찌', '##나', '우렁', '##찬', '##지', '밖', '##에서', '소리', '##를', '지르', '##면', '우리', '집', '##이', '쿵', '##쿵', '울리', '##는', '것', '같', '##다', '.', '우리', '##가', '이사', '오', '##던', '날', '##부터', '이삿짐', '나르', '##는', '사다리차', '##가', '시끄럽', '##다고', '난리', '##였', '##다', '.', '“', '허구', '##헌', '날', '다', '나', '##뚜', '##고', '우', '##짠', '일', '##로', '이', '늦', '##은', '시간', '##에', '이사', '##를', '해', '쌌', '##소', '?', '”', '“', '죄송', '##해요', '.', '저희', '부부', '##가', '다', '일', '##을', '다']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 단어가 형태소 단위로 쪼개졌음을 확인할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. token to ids 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32506, 64805, 10739, 37723, 56104, 69343, 101325, 100851, 92802, 46304, 106505, 90637, 247, 22573, 38265, 12255, 104236, 63873, 11244, 54917, 63873, 25616, 85189, 69343, 41244, 18166, 37980, 36266, 55635, 37544, 30178, 41251, 50535, 69343, 36058, 59230, 12021, 66682, 39803, 108833, 38265, 108833, 12694, 26719, 48502, 46190, 94313, 70242, 59230, 96141, 44426, 66101, 7286, 74951, 38485, 51266, 104907, 54917, 86314, 100851, 84263, 97360, 91440, 75291, 68324, 59230, 37544, 30178, 66682, 38110, 101743, 10739, 95738, 43717, 111844, 59230, 82805, 12559, 91440, 4991, 43616, 102258, 8966, 41251, 57686, 22573, 65141, 106126, 46190, 3442, 15996, 2384, 91451, 55253, 13773, 38110, 74951, 54917, 17299, 79701, 85216, 66682, 60654, 77234, 22935, 59230, 38110, 41251, 104975, 20921, 27222, 32908, 11141, 55892, 94060, 66682, 22287, 41251, 46304, 106505, 104914, 54136, 22935, 59230, 32506, 65459, 102738, 32908, 81775, 12021, 45717, 47248, 76757, 18471, 52529, 110598, 37723, 56489, 102004, 103593, 38485, 104975, 91451, 107182, 21764, 56512, 63873, 25616, 32506, 41395, 69832, 59230, 45093, 1514, 41251, 81775, 52529, 69343, 81775]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 본 모든 단어 토큰들이 id로 변환됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. 전체 텍스트 데이터를 id로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14548850"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(data)\n",
    "\n",
    "data_token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "len(data_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 약 58Mb 의 데이터를 tokenizing 하고, 그걸 다시 id로 변환하는 작업\n",
    "\n",
    "\n",
    "- 위 작업은 약 3분 정도 소요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. 변환된 `data_token_ids`를 `hdf5` 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `lc39_data_.hdf5` 명으로 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './common/data/lc39_data_1.hdf5'\n",
    "\n",
    "f = h5py.File(save_path, 'w')\n",
    "g = f.create_group(\"data\")\n",
    "g.create_dataset(\"crawled_geulteen\", data=data_token_ids)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. 저장된 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"lc39_data_1.hdf5\" (mode r)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = h5py.File(save_path, 'r')\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['data']>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"crawled_geulteen\": shape (14548850,), type \"<i8\">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi['data']['crawled_geulteen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 32506,  64805,  10739,  37723,  56104,  69343, 101325, 100851,\n",
       "        92802,  46304, 106505,  90637,    247,  22573,  38265,  12255,\n",
       "       104236,  63873,  11244,  54917,  63873,  25616,  85189,  69343,\n",
       "        41244,  18166,  37980,  36266,  55635,  37544,  30178,  41251,\n",
       "        50535,  69343,  36058,  59230,  12021,  66682,  39803, 108833,\n",
       "        38265, 108833,  12694,  26719,  48502,  46190,  94313,  70242,\n",
       "        59230,  96141,  44426,  66101,   7286,  74951,  38485,  51266,\n",
       "       104907,  54917,  86314, 100851,  84263,  97360,  91440,  75291,\n",
       "        68324,  59230,  37544,  30178,  66682,  38110, 101743,  10739,\n",
       "        95738,  43717, 111844,  59230,  82805,  12559,  91440,   4991,\n",
       "        43616, 102258,   8966,  41251,  57686,  22573,  65141, 106126,\n",
       "        46190,   3442,  15996,   2384,  91451,  55253,  13773,  38110,\n",
       "        74951,  54917,  17299,  79701])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi['data']['crawled_geulteen'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 14,548,850 길이의 데이터 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
